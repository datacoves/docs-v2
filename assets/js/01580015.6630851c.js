"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1791],{28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>i});var s=n(96540);const a={},r=s.createContext(a);function o(e){const t=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(r.Provider,{value:t},e.children)}},84632:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"how-tos/airflow/test-dags","title":"Test dags","description":"In Datacoves you can easily test your Airflow DAGs using pytest in the command line. However you can also run these validations in your CI/CD pipeline.","source":"@site/docs/how-tos/airflow/test-dags.md","sourceDirName":"how-tos/airflow","slug":"/how-tos/airflow/test-dags","permalink":"/docusaurus-test/docs/how-tos/airflow/test-dags","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/how-tos/airflow/test-dags.md","tags":[],"version":"current","sidebarPosition":23,"frontMatter":{"title":"Test dags","sidebar_position":23},"sidebar":"tutorialSidebar","previous":{"title":"Sync database","permalink":"/docusaurus-test/docs/how-tos/airflow/sync-database"},"next":{"title":"Use airflow api","permalink":"/docusaurus-test/docs/how-tos/airflow/use-airflow-api"}}');var a=n(74848),r=n(28453);const o={title:"Test dags",sidebar_position:23},i="Run DAG tests in your CI/CD",l={},d=[{value:"Step 1: Create your pytest validations file in the <code>orchestrate/test_dags</code> directory.",id:"step-1-create-your-pytest-validations-file-in-the-orchestratetest_dags-directory",level:3},{value:"Step 2 Add the <code>conftest.py</code> file to your <code>orchestrate/test_dags</code> directory.",id:"step-2-add-the-conftestpy-file-to-your-orchestratetest_dags-directory",level:3},{value:"Step 3: Add the following file to your github actions.",id:"step-3-add-the-following-file-to-your-github-actions",level:3}];function c(e){const t={a:"a",code:"code",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"run-dag-tests-in-your-cicd",children:"Run DAG tests in your CI/CD"})}),"\n",(0,a.jsxs)(t.p,{children:["In Datacoves you can easily test your Airflow DAGs using ",(0,a.jsx)(t.a,{href:"/docusaurus-test/docs/reference/airflow/datacoves-commands#datacoves-my-pytest",children:"pytest"})," in the command line. However you can also run these validations in your CI/CD pipeline."]}),"\n",(0,a.jsx)(t.p,{children:"To do this follow these steps:"}),"\n",(0,a.jsxs)(t.h3,{id:"step-1-create-your-pytest-validations-file-in-the-orchestratetest_dags-directory",children:["Step 1: Create your pytest validations file in the ",(0,a.jsx)(t.code,{children:"orchestrate/test_dags"})," directory."]}),"\n",(0,a.jsx)(t.p,{children:"Here is an example script:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# orchestrate/test_dags/validate_dags.py\n"""Example DAGs test. This test ensures that all Dags have tags, retries set to two, and no import errors.\nThis is an example pytest and may not be fit the context of your DAGs. Feel free to add and remove tests."""\n\nimport os\nimport logging\nfrom contextlib import contextmanager\nimport pytest\nimport warnings\nfrom airflow.models import DagBag\n\nAPPROVED_TAGS = {\'extract_and_load\',\n                \'transform\',\n                \'python_script\',\n                \'ms_teams_notification\',\n                \'slack_notification\',\n                \'marketing_automation\',\n                \'update_catalog\',\n                \'parameters\',\n                \'sample\'}\n\nALLOWED_OPERATORS = [\n    "_PythonDecoratedOperator",  # this allows the @task decorator\n    "DatacovesBashOperator",\n    "DatacovesDbtOperator",\n    "DatacovesDataSyncOperatorSnowflake",\n    "_DatacovesDataSyncSnowflakeDecoratedOperator",\n    "_DatacovesDataSyncRedshiftDecoratedOperator",\n    "AirbyteTriggerSyncOperator",\n    \'FivetranOperator\',\n    \'FivetranSensor\',\n]\n\n@contextmanager\ndef suppress_logging(namespace):\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True\n    try:\n        yield\n    finally:\n        logger.disabled = old_value\n\n### Custom tests start here ###\ndef get_import_errors():\n    """\n    Generate a tuple for import errors in the dag bag\n    """\n    with suppress_logging("airflow"):\n        dag_bag = DagBag(include_examples=False)\n\n        def strip_path_prefix(path):\n            return os.path.relpath(path, os.environ.get("AIRFLOW_HOME"))\n\n        # prepend "(None,None)" to ensure that a test object is always created even if it\'s a no op.\n        return [(None, None)] + [\n            (strip_path_prefix(k), v.strip()) for k, v in dag_bag.import_errors.items()\n        ]\n\n\ndef get_dags():\n    """\n    Generate a tuple of dag_id, <DAG objects> in the DagBag\n    """\n    with suppress_logging("airflow"):\n        dag_bag = DagBag(include_examples=False)\n\n    def strip_path_prefix(path):\n        return os.path.relpath(path, os.environ.get("AIRFLOW__CORE__DAGS_FOLDER"))\n\n    return [(k, v, strip_path_prefix(v.fileloc)) for k, v in dag_bag.dags.items()]\n\n\n@pytest.mark.parametrize(\n    "rel_path,rv", get_import_errors(), ids=[x[0] for x in get_import_errors()]\n)\ndef test_file_imports(rel_path, rv):\n    """Test for import errors on a file"""\n    if rel_path and rv:\n        raise Exception(f"{rel_path} failed to import with message \\n {rv}")\n\n\n\n@pytest.mark.parametrize(\n    "dag_id,dag,fileloc", get_dags(), ids=[x[2] for x in get_dags()]\n)\ndef test_dag_tags(dag_id, dag, fileloc):\n    """\n    test if a DAG is tagged and if TAGs are in the approved list\n    """\n    assert dag.tags, f"{dag_id} in {fileloc} has no tags"\n    if APPROVED_TAGS:\n        assert not set(dag.tags) - APPROVED_TAGS\n\n\n@pytest.mark.parametrize(\n    "dag_id,dag, fileloc", get_dags(), ids=[x[2] for x in get_dags()]\n)\ndef test_dag_has_catchup_false(dag_id, dag, fileloc):\n    """\n    test if a DAG has catchup set to False\n    """\n    assert (\n        dag.catchup == False\n    ), f"{dag_id} in {fileloc} must have catchup set to False."\n\n\n\n@pytest.mark.parametrize(\n    "dag_id, dag, fileloc", get_dags(), ids=[x[0] for x in get_dags()]\n)\ndef test_dag_uses_allowed_operators_only(dag_id, dag, fileloc):\n    """\n    Test if a DAG uses only allowed operators.\n    """\n    for task in dag.tasks:\n        assert any(\n            task.task_type == allowed_op for allowed_op in ALLOWED_OPERATORS\n        ), f"{task.task_id} in {dag_id} ({fileloc}) uses {task.task_type}, which is not in the list of allowed operators."\n\n\n@pytest.mark.parametrize(\n    "dag_id,dag, fileloc", get_dags(), ids=[x[2] for x in get_dags()]\n)\ndef test_dag_retries(dag_id, dag, fileloc):\n    """\n    test if a DAG has retries set\n    """\n    num_retries = dag.default_args.get("retries", 0)\n\n    if num_retries == 0 or num_retries is None:\n        pytest.fail(f"{dag_id} in {fileloc} must have task retries >= 1 it currently has {num_retries}.")\n    elif num_retries < 3:\n        warnings.warn(f"{dag_id} in {fileloc} should have task retries >= 3 it currently has {num_retries}.", UserWarning)\n    else:\n        assert num_retries >= 3, f"{dag_id} in {fileloc} must have task retries >= 2 it currently has {num_retries}."\n'})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Summary"})}),"\n",(0,a.jsx)(t.p,{children:"This file defines a set of pytest-based validation tests for Airflow DAGs. It ensures that:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"DAG Import Validation \u2013 Detects and reports any import errors in DAG files."}),"\n",(0,a.jsx)(t.li,{children:"Tag Compliance \u2013 Checks that all DAGs have at least one tag and that tags are within an approved list."}),"\n",(0,a.jsx)(t.li,{children:"Catchup Settings \u2013 Ensures that all DAGs have catchup set to False to prevent unintended backfills."}),"\n",(0,a.jsx)(t.li,{children:"Operator Validation \u2013 Restricts DAGs to use only a predefined set of allowed operators."}),"\n",(0,a.jsx)(t.li,{children:"Retry Configuration \u2013 Validates that DAGs have a retry setting of at least 1 and warns if it is less than 3."}),"\n"]}),"\n",(0,a.jsxs)(t.h3,{id:"step-2-add-the-conftestpy-file-to-your-orchestratetest_dags-directory",children:["Step 2 Add the ",(0,a.jsx)(t.code,{children:"conftest.py"})," file to your ",(0,a.jsx)(t.code,{children:"orchestrate/test_dags"})," directory."]}),"\n",(0,a.jsxs)(t.p,{children:["This file will import custom tests that the Datacoves team has created such as validating ",(0,a.jsx)(t.a,{href:"/how-tos/airflow/use-aws-secrets-manager.md",children:"variable calls are not made at the highest level"})," of a DAG."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# orchestrate/test_dags/conftest.py\nfrom datacoves_airflow_provider.testing.custom_reporter import *\n"})}),"\n",(0,a.jsx)(t.h3,{id:"step-3-add-the-following-file-to-your-github-actions",children:"Step 3: Add the following file to your github actions."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-yaml",children:'# 10_integrate_airflow_changes.yml\nname: \ud83c\udfaf Airflow Validations\n\non:  # yamllint disable-line rule:truthy\n  pull_request:\n    paths:\n      - orchestrate/*\n      - orchestrate/**/*\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# This cancels a run if another change is pushed to the same branch\nconcurrency:\n  group: orchestrate-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  airflow:\n    name: Pull Request Airflow Tests\n    runs-on: ubuntu-latest\n    # container: datacoves/ci-airflow-dbt-snowflake:3.3\n    container: datacoves/ci-airflow-dbt-snowflake:3.3\n\n    env:\n      AIRFLOW__CORE__DAGS_FOLDER: /__w/${{ github.event.repository.name }}/${{ github.event.repository.name }}/orchestrate/dags\n      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 300\n      AIRFLOW__ARTIFACTS_PATH: /__w/${{ github.event.repository.name }}/${{ github.event.repository.name }}/orchestrate\n      DATACOVES__DBT_HOME: /__w/${{ github.event.repository.name }}/${{ github.event.repository.name }}/transform\n      DATACOVES__REPO_PATH: /__w/${{ github.event.repository.name }}/${{ github.event.repository.name }}\n      PYTHONPATH: /__w/${{ github.event.repository.name }}/${{ github.event.repository.name }}\n      FORCE_COLOR: 1\n      OUTPUT_FILE: /__w/${{ github.event.repository.name }}/${{ github.event.repository.name }}/test_output.md\n\n    steps:\n      - name: Checkout branch\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          ref: ${{ github.event.pull_request.head.sha }}\n\n      - name: Test DAGs Load time and variable usage at top level\n        id: test_load_time_variables\n        run: python /usr/app/test_dags.py --dag-loadtime-threshold 1 --check-variable-usage --write-output --filename "$OUTPUT_FILE"\n\n      # if write-output is set in the prior step, the following step will run\n      - name: Add PR comment of results of test_load_time_variables tests\n        uses: thollander/actions-comment-pull-request@v2\n        with:\n          filePath: ${{ env.OUTPUT_FILE }}\n          comment_tag: Test DAGs Load time and variable usage at top level\n\n      - name: Custom Airflow Validation Tests\n        env:\n          NO_COLOR: 1\n        run: pytest $AIRFLOW__ARTIFACTS_PATH/test_dags/validate_dags.py --output-file "$OUTPUT_FILE"\n\n\n      - name: Add PR comment of results of custom Airflow validation tests\n        if: always()\n        uses: thollander/actions-comment-pull-request@v2\n        with:\n          # filePath: formatted_output.md\n          filePath: ${{ env.OUTPUT_FILE }}\n          comment_tag: Custom Tests\n          GITHUB_TOKEN: ${{ github.token }}\n'})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Summary"})}),"\n",(0,a.jsx)(t.p,{children:"This GitHub Actions workflow automatically:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Triggers when changes are made to the orchestrate directory."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Tests for variable usage at the top level in DAGs to prevent costly issues."}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:"Runs custom validation tests and comments results on the PR."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"By integrating this workflow, you can ensure Airflow DAGs meet quality standards before deployment."})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);