"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3298],{4155:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/s3_test_table_schema-75fb380e0baf513429a151e9b78f0e21.jpg"},28453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>l});var n=a(96540);const s={},o=n.createContext(s);function i(e){const t=n.useContext(o);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(o.Provider,{value:t},e.children)}},29164:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/json_variant_table-8b0d192a0ff4089e65a17e020c962495.jpg"},31382:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/s3_test_table_additional_col-8b9d11e3809b5870c11b19e426333853.jpg"},39302:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"how-tos/airflow/s3-to-snowflake","title":"DAGS - Load from S3 to Snowflake","description":"Schema Evolution","source":"@site/docs/how-tos/airflow/s3-to-snowflake.md","sourceDirName":"how-tos/airflow","slug":"/how-tos/airflow/s3-to-snowflake","permalink":"/docusaurus-test/docs/how-tos/airflow/s3-to-snowflake","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/how-tos/airflow/s3-to-snowflake.md","tags":[],"version":"current","sidebarPosition":25,"frontMatter":{"title":"DAGS - Load from S3 to Snowflake","sidebar_position":25},"sidebar":"tutorialSidebar","previous":{"title":"DAGs - Get Current Git Branch Name from a DAG Task","permalink":"/docusaurus-test/docs/how-tos/airflow/get-current-branch-name"},"next":{"title":"Use datacoves secrets manager","permalink":"/docusaurus-test/docs/how-tos/airflow/use-datacoves-secrets-manager"}}');var s=a(74848),o=a(28453);const i={title:"DAGS - Load from S3 to Snowflake",sidebar_position:25},l="Loading S3 Files into Snowflake",r={},c=[{value:"Schema Evolution",id:"schema-evolution",level:2},{value:"Caveats and limitations",id:"caveats-and-limitations",level:3},{value:"Loading JSON data into a variant column",id:"loading-json-data-into-a-variant-column",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"loading-s3-files-into-snowflake",children:"Loading S3 Files into Snowflake"})}),"\n",(0,s.jsx)(t.h2,{id:"schema-evolution",children:"Schema Evolution"}),"\n",(0,s.jsxs)(t.p,{children:["Snowflake has a built-in feature for ",(0,s.jsx)(t.a,{href:"https://docs.snowflake.com/en/user-guide/data-load-schema-evolution",children:"schema evolution"}),". It allows us to create the initial table using a file as a template and then it will automatically handle any changes to the schema going forward."]}),"\n",(0,s.jsx)(t.h3,{id:"caveats-and-limitations",children:"Caveats and limitations"}),"\n",(0,s.jsxs)(t.p,{children:["Schema evolution is ",(0,s.jsx)(t.a,{href:"https://docs.snowflake.com/en/user-guide/data-load-schema-evolution#usage-notes",children:"limited to a change that adds a maximum of 10 columns or evolving no more than 1 schema per COPY operation"})]}),"\n",(0,s.jsxs)(t.p,{children:["You cannot go from a column containing ",(0,s.jsx)(t.code,{children:"NUMBER"})," values to ",(0,s.jsx)(t.code,{children:"VARCHAR"}),", but you can go from ",(0,s.jsx)(t.code,{children:"VARCHAR"})," to ",(0,s.jsx)(t.code,{children:"NUMBER"})," as it will insert it as ",(0,s.jsx)(t.code,{children:"VARCHAR"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 1:"})," In a Snowflake SQL worksheet Navigate to the database and schema where you will be loading the data to."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 2:"})," Create a custom file format for the data that you are planning to load. In this example we are using a custom CSV file format."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE OR REPLACE FILE FORMAT MY_CSV_FORMAT\nTYPE = 'CSV' \nPARSE_HEADER = TRUE\nERROR_ON_COLUMN_COUNT_MISMATCH = FALSE;\n"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 3:"})," Now we will create a Snowflake stage and link our custom file format to it."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE OR REPLACE STAGE TEST_STAGE\n    FILE_FORMAT = MY_CSV_FORMAT;\n"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 4:"})," We are now going to upload a csv file to this Snowflake stage that we will use as a template to create the table where data will be loaded. You can do this via the Snowflake UI or by using the SnowSQL command line tool."]}),"\n",(0,s.jsx)(t.p,{children:"This example will use SnowSQL and our data looks like this:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Sample data",src:a(43619).A+"",width:"450",height:"356"})}),"\n",(0,s.jsx)(t.p,{children:"The SnowSQL command to upload the file to the Snowflake stage is below:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"PUT file://test_file.csv @TEST_STAGE;\n"})}),"\n",(0,s.jsx)(t.p,{children:"Where test_file.csv is in the same folder from where we are running the SnowSQL command."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 5:"})," Once the file has been uploaded to the test stage, we create the initial table using it as a template and enabling schema evolution."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE OR REPLACE TABLE TEST_TABLE\n    ENABLE_SCHEMA_EVOLUTION== TRUE\n    USING TEMPLATE (\n        SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n            FROM TABLE(\n                INFER_SCHEMA(\n                    LOCATION->'@TEST_STAGE/test_file.csv.gz,\n                    FILE_FORMAT=>'my_csv_format'\n                )\n            )\n    );\n"})}),"\n",(0,s.jsx)(t.p,{children:"The TEST_TABLE schema now looks like this:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"test_table",src:a(4155).A+"",width:"1198",height:"278"})}),"\n",(0,s.jsx)(t.p,{children:"However, the table does not have any data loaded into it."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Empty test_table",src:a(47695).A+"",width:"1198",height:"168"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 6:"})," To load the data from the file we used as a template we use the following COPY INTO SQL."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"COPY INTO TEST_TABLE\n    FROM '@TEST_STAGE/test_file.csv.gz'\n    FILE_FORMAT =  'my_csv_format'\n    MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;\n"})}),"\n",(0,s.jsx)(t.p,{children:"And we can now see the data in the table:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"test_table copied",src:a(80778).A+"",width:"1198",height:"286"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 7:"})," Now we\u2019re going to load another file into TEST_TABLE that has an additional column."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Test data additional column",src:a(31382).A+"",width:"548",height:"362"})}),"\n",(0,s.jsx)(t.p,{children:"Again, we will use the SnowSQL PUT command seen below:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"PUT files://test_file_copy.csv @TEST_STAGE;\n"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 8:"})," Now we can run another COPY INTO statement that references the new file."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"COPY INTO TEST_TABLE\n    FROM '@TEST_STAGE/test_file_copy.csv.gz'\n    FILE_FORMAT = 'my_csv_format'\n    MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;\n"})}),"\n",(0,s.jsx)(t.p,{children:"And now the table has an additional column called COUNTRY_CODE:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"test_table additional column",src:a(81483).A+"",width:"1192",height:"384"})}),"\n",(0,s.jsx)(t.h2,{id:"loading-json-data-into-a-variant-column",children:"Loading JSON data into a variant column"}),"\n",(0,s.jsx)(t.p,{children:"If the data that you want to load is in JSON format and the schema is likely to change then a recommended pattern is to load the JSON into a single Snowflake variant column. This allows you to parse out and model the data downstream without having to worry about the change in schema."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 1:"})," 1. Create a Snowflake table with a single variant column."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE OR REPLACE TABLE VARIABLE_TABLE (MAIN_COLUMN VARIANT);\n"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 2:"}),"  Now create a custom file format for the JSON data."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE OR REPLACE FILE FORMAT MY_JSON_FORMAT\n    TYPE = 'JSON'\n    STRIP_OUTER_ARRAY = TRUE;\n"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 3:"})," After this we create a Snowflake stage that uses the JSON custom file format."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE OR REPLACE STAGE JSON_STAGE\n    FILE_FORMAT = MY_JSON_FORMAT;\n"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 4:"})," We can now load JSON files that have been staged using the following COPY INTO command."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"COPY INTO VARIANT_TABLE \n    FROM @JSON_STAGE\n"})}),"\n",(0,s.jsx)(t.p,{children:"Our variant table now looks like this:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"json variant table",src:a(29164).A+"",width:"1184",height:"448"})})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},43619:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/s3_sample_data-bc17a19907fc5377b3bc073850a8b228.jpg"},47695:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/s3_test_table_empty-5dc687f068e7174675d572d5dccbe92e.jpg"},80778:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/s3_test_table_copied-ddc3265e09b69e131090ce129416f5ab.jpg"},81483:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/s3_test_table_additional_call_snowflake-43ebfe8d97e64f3a9063b818ebb3777f.jpg"}}]);