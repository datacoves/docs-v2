"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4380],{28453:(e,t,r)=>{r.d(t,{R:()=>i,x:()=>a});var n=r(96540);const s={},o=n.createContext(s);function i(e){const t=n.useContext(o);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(o.Provider,{value:t},e.children)}},70085:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"getting-started/Admin/configure-repository","title":"Configure Git Repository","description":"Now that you have configured your Airflow settings you must ensure that your repository has the correct folder structure to pick up the DAGs we create. You will need to add folders to your project repository in order to match the folder defaults we just configured for Airflow. These folders are orchestrate/dags and, optionally, orchestrate/dagsymldefinitions.","source":"@site/docs/getting-started/Admin/configure-repository.md","sourceDirName":"getting-started/Admin","slug":"/getting-started/Admin/configure-repository","permalink":"/docusaurus-test/docs/getting-started/Admin/configure-repository","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Configure Git Repository","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Configure Airflow","permalink":"/docusaurus-test/docs/getting-started/Admin/configure-airflow"},"next":{"title":"Configure Git Repository Using dbt-coves","permalink":"/docusaurus-test/docs/getting-started/Admin/configure-repository-using-dbt-coves"}}');var s=r(74848),o=r(28453);const i={title:"Configure Git Repository",sidebar_position:3},a="Update Repository for Airflow",d={},l=[{value:"Create a profiles.yml",id:"create-a-profilesyml",level:2},{value:"Snowflake",id:"snowflake",level:3},{value:"Redshift",id:"redshift",level:3},{value:"BigQuery",id:"bigquery",level:3},{value:"Databricks",id:"databricks",level:3},{value:"Getting Started Next Steps",id:"getting-started-next-steps",level:2}];function c(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"update-repository-for-airflow",children:"Update Repository for Airflow"})}),"\n",(0,s.jsxs)(t.p,{children:["Now that you have configured your Airflow settings you must ensure that your repository has the correct folder structure to pick up the DAGs we create. You will need to add folders to your project repository in order to match the folder defaults we just configured for Airflow. These folders are ",(0,s.jsx)(t.code,{children:"orchestrate/dags"})," and, optionally, ",(0,s.jsx)(t.code,{children:"orchestrate/dags_yml_definitions"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 1:"})," Add a folder named ",(0,s.jsx)(t.code,{children:"orchestrate"})," and a folder inside ",(0,s.jsx)(t.code,{children:"orchestrate"})," named ",(0,s.jsx)(t.code,{children:"dags"}),". ",(0,s.jsx)(t.code,{children:"orchestrate/dags"})," is where you will be placing your DAGs as defined earlier in our Airflow settings with the  ",(0,s.jsx)(t.code,{children:"Python DAGs path"})," field."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 2:"})," ",(0,s.jsx)(t.strong,{children:"ONLY If using Git Sync"}),". If you have not already done so, create a branch named ",(0,s.jsx)(t.code,{children:"airflow_development"})," from ",(0,s.jsx)(t.code,{children:"main"}),". This branch was defined as the sync branch earlier in our Airflow Settings with the ",(0,s.jsx)(t.code,{children:"Git branch name"})," field. Best practice will be to keep this branch up-to-date with ",(0,s.jsx)(t.code,{children:"main"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 3:"})," ",(0,s.jsx)(t.strong,{children:"This step is optional"})," if you would like to make use of the ",(0,s.jsx)(t.a,{href:"https://github.com/datacoves/dbt-coves?tab=readme-ov-file#airflow-dags-generation-arguments",children:"dbt-coves"})," ",(0,s.jsx)(t.code,{children:"dbt-coves generate airflow-dags"})," command. Create the ",(0,s.jsx)(t.code,{children:"dags_yml_definitions"})," folder inside of your newly created ",(0,s.jsx)(t.code,{children:"orchestrate"})," folder. This will leave you with two folders inside ",(0,s.jsx)(t.code,{children:"orchestrate"}),"- ",(0,s.jsx)(t.code,{children:"orchestrate/dags"})," and ",(0,s.jsx)(t.code,{children:"orchestrate/dags_yml_definitions"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 4:"})," ",(0,s.jsx)(t.strong,{children:"This step is optional"})," if you would like to make use of the dbt-coves' extension ",(0,s.jsx)(t.code,{children:"dbt-coves generate airflow-dags"})," command. You must create a config file for dbt-coves. Please follow the ",(0,s.jsx)(t.a,{href:"/docusaurus-test/docs/how-tos/airflow/generate-dags-from-yml",children:"generate DAGs from yml"})," docs."]}),"\n",(0,s.jsx)(t.h2,{id:"create-a-profilesyml",children:"Create a profiles.yml"}),"\n",(0,s.jsxs)(t.p,{children:["If the ",(0,s.jsx)(t.code,{children:"delivery mode"})," of your service connection is ",(0,s.jsx)(t.a,{href:"/docusaurus-test/docs/how-tos/datacoves/how_to_service_connections",children:(0,s.jsx)(t.code,{children:"Environment Variables"})})," then Airflow will need a ",(0,s.jsx)(t.code,{children:"profiles.yml"}),". The available environment variables will vary based on your data warehouse. We have made it simple to set this up by completing the following steps. This profiles.yml will also be used in the CI/CD process."]}),"\n",(0,s.jsxs)(t.p,{children:["To create your and your ",(0,s.jsx)(t.code,{children:"profiles.yml"}),":"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 1:"})," Create the ",(0,s.jsx)(t.code,{children:"automate"})," folder at the root of your project"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 2:"})," Create the ",(0,s.jsx)(t.code,{children:"dbt"})," folder inside the ",(0,s.jsx)(t.code,{children:"automate"})," folder"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 3:"})," Create the ",(0,s.jsx)(t.code,{children:"profiles.yml"})," inside of your ",(0,s.jsx)(t.code,{children:"automate"})," folder. ie) ",(0,s.jsx)(t.code,{children:"automate/dbt/profiles.yml"})]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Step 4:"})," Copy the following configuration into your ",(0,s.jsx)(t.code,{children:"profiles.yml"})]}),"\n",(0,s.jsx)(t.h3,{id:"snowflake",children:"Snowflake"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-yaml",children:"default:\n  target: default_target\n  outputs:\n    default_target:\n      type: snowflake\n      threads: 8\n      client_session_keep_alive: true\n\n      account: \"{{ env_var('DATACOVES__MAIN__ACCOUNT') }}\"\n      database: \"{{ env_var('DATACOVES__MAIN__DATABASE') }}\"\n      schema: \"{{ env_var('DATACOVES__MAIN__SCHEMA') }}\"\n      user: \"{{ env_var('DATACOVES__MAIN__USER') }}\"\n      password: \"{{ env_var('DATACOVES__MAIN__PASSWORD') }}\"\n      role: \"{{ env_var('DATACOVES__MAIN__ROLE') }}\"\n      warehouse: \"{{ env_var('DATACOVES__MAIN__WAREHOUSE') }}\"\n"})}),"\n",(0,s.jsx)(t.h3,{id:"redshift",children:"Redshift"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-yaml",children:"company-name:\n  target: dev\n  outputs:\n    dev:\n      type: redshift\n      host: \"{{ env_var('DATACOVES__MAIN__HOST') }}\"\n      user: \"{{ env_var('DATACOVES__MAIN__USER') }}\"\n      password: \"{{ env_var('DATACOVES__MAIN__PASSWORD') }}\"\n      dbname: \"{{ env_var('DATACOVES__MAIN__DATABASE') }}\"\n      schema: analytics\n      port: 5439\n"})}),"\n",(0,s.jsx)(t.h3,{id:"bigquery",children:"BigQuery"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-yaml",children:"my-bigquery-db:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: service-account\n      project: GCP_PROJECT_ID\n      dataset:  \"{{ env_var('DATACOVES__MAIN__DATASET') }}\"\n      threads: 4 # Must be a value of 1 or greater\n      keyfile:  \"{{ env_var('DATACOVES__MAIN__KEYFILE_JSON') }}\"\n"})}),"\n",(0,s.jsx)(t.h3,{id:"databricks",children:"Databricks"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-yaml",children:"your_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: databricks\n      catalog: [optional catalog name if you are using Unity Catalog]\n      schema: \"{{ env_var('DATACOVES__MAIN__SCHEMA') }}\" # Required\n      host: \"{{ env_var('DATACOVES__MAIN__HOST') }}\" # Required\n      http_path: \"{{ env_var('DATACOVES__MAIN__HTTP_PATH') }}\" # Required\n      token: \"{{ env_var('DATACOVES__MAIN__TOKEN') }}\" # Required Personal Access Token (PAT) if using token-based authentication\n      threads: 4 \n"})}),"\n",(0,s.jsx)(t.h2,{id:"getting-started-next-steps",children:"Getting Started Next Steps"}),"\n",(0,s.jsx)(t.p,{children:"You will want to set up notifications. Selet the option that works best for your organization."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Email:"})," ",(0,s.jsx)(t.a,{href:"/docusaurus-test/docs/how-tos/airflow/send-emails",children:"Setup Email Integration"})]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"MS Teams:"})," ",(0,s.jsx)(t.a,{href:"/docs/how-tos/airflow/send-ms-teams-notifications",children:"Setup MS Teams Integration"})]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Slack:"})," ",(0,s.jsx)(t.a,{href:"/docs/how-tos/airflow/send-slack-notifications",children:"Setup Slack Integration"})]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);