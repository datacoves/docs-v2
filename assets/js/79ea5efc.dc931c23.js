"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1074],{28453:(e,o,n)=>{n.d(o,{R:()=>s,x:()=>i});var r=n(96540);const a={},t=r.createContext(a);function s(e){const o=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function i(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(t.Provider,{value:o},e.children)}},79438:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"reference/airflow/datacoves-operator","title":"Datacoves operator","description":"[!NOTE] All operators use Datacoves Service connections with Delivery Mode set to Environment Variables","source":"@site/docs/reference/airflow/datacoves-operator.md","sourceDirName":"reference/airflow","slug":"/reference/airflow/datacoves-operator","permalink":"/docusaurus-test/docs/reference/airflow/datacoves-operator","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/reference/airflow/datacoves-operator.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Datacoves operator","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Datacoves decorators","permalink":"/docusaurus-test/docs/reference/airflow/datacoves-decorators"},"next":{"title":"Environment service connection vars","permalink":"/docusaurus-test/docs/reference/airflow/environment-service-connection-vars"}}');var a=n(74848),t=n(28453);const s={title:"Datacoves operator",sidebar_position:7},i="Datacoves Operators & Generators",c={},d=[{value:"Datacoves Bash Operator",id:"datacoves-bash-operator",level:2},{value:"Datacoves dbt Operator",id:"datacoves-dbt-operator",level:2},{value:"Data Sync Operators",id:"data-sync-operators",level:2}];function l(e){const o={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(o.header,{children:(0,a.jsx)(o.h1,{id:"datacoves-operators--generators",children:"Datacoves Operators & Generators"})}),"\n",(0,a.jsxs)(o.blockquote,{children:["\n",(0,a.jsxs)(o.p,{children:["[!NOTE] All operators use Datacoves Service connections with ",(0,a.jsx)(o.code,{children:"Delivery Mode"})," set to ",(0,a.jsx)(o.code,{children:"Environment Variables"})]}),"\n"]}),"\n",(0,a.jsx)(o.p,{children:"When utilizing dbt-coves to generate DAGs, it's crucial to grasp the functionality of the two frequently used operators and their behind-the-scenes operations, enhancing your Airflow experience."}),"\n",(0,a.jsx)(o.h2,{id:"datacoves-bash-operator",children:"Datacoves Bash Operator"}),"\n",(0,a.jsx)(o.pre,{children:(0,a.jsx)(o.code,{children:"from operators.datacoves.bash import DatacovesBashOperator \n"})}),"\n",(0,a.jsx)(o.p,{children:"This custom operator is an extension of Airflow's default Bash Operator. It:"}),"\n",(0,a.jsxs)(o.ul,{children:["\n",(0,a.jsxs)(o.li,{children:["Copies the entire Datacoves repo to a temporary directory, to avoid read-only errors when running ",(0,a.jsx)(o.code,{children:"bash_command"})]}),"\n",(0,a.jsx)(o.li,{children:"Activates the Datacoves Airflow virtualenv"}),"\n",(0,a.jsxs)(o.li,{children:["Runs the command in the repository root (or a passed ",(0,a.jsx)(o.code,{children:"cwd"}),", relative path from repo root where to run command from)"]}),"\n"]}),"\n",(0,a.jsx)(o.p,{children:"Params:"}),"\n",(0,a.jsxs)(o.ul,{children:["\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"bash_command"}),": command to run"]}),"\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"cwd"})," (optional): relative path from repo root where to run command from"]}),"\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"activate_venv"})," (optional): whether to activate the Datacoves Airflow virtualenv or not"]}),"\n"]}),"\n",(0,a.jsx)(o.pre,{children:(0,a.jsx)(o.code,{className:"language-python",children:'"""## Simple Datacoves DAG\nThis DAG executes a Python script using DatacovesBashOperator.\n"""\n\nfrom airflow.decorators import dag\nfrom operators.datacoves.bash import DatacovesBashOperator\nfrom pendulum import datetime\n\n@dag(\n    doc_md=__doc__,\n    default_args={\n        "start_date": datetime(2022, 10, 10),\n        "owner": "Noel Gomez",\n        "email": "gomezn@example.com",\n        "email_on_failure": True,\n        "retries": 3,\n    },\n    catchup=False,\n    tags=["python_script"],\n    description="Simple Datacoves DAG",\n    schedule="0 0 1 */12 *",\n)\ndef simple_datacoves_dag():\n    run_python_script = DatacovesBashOperator(\n        task_id="run_python_script",\n        bash_command="python orchestrate/python_scripts/sample_script.py",\n    )\n\nsimple_datacoves_dag()\n'})}),"\n",(0,a.jsx)(o.h2,{id:"datacoves-dbt-operator",children:"Datacoves dbt Operator"}),"\n",(0,a.jsxs)(o.blockquote,{children:["\n",(0,a.jsxs)(o.p,{children:["[!WARNING]If you have either ",(0,a.jsx)(o.code,{children:"dbt_modules"})," or ",(0,a.jsx)(o.code,{children:"dbt_packages"})," folders in your project repo Datacoves won't run ",(0,a.jsx)(o.code,{children:"dbt deps"}),"."]}),"\n"]}),"\n",(0,a.jsx)(o.pre,{children:(0,a.jsx)(o.code,{children:"from operators.datacoves.dbt import DatacovesDbtOperator\n"})}),"\n",(0,a.jsx)(o.p,{children:"This custom operator is an extension of Datacoves Bash Operator and simplifies running dbt commands within Airflow.\nThe operator does the following:"}),"\n",(0,a.jsxs)(o.ul,{children:["\n",(0,a.jsxs)(o.li,{children:["Copies the entire Datacoves repo to a temporary directory, to avoid read-only errors when running ",(0,a.jsx)(o.code,{children:"bash_command"}),"."]}),"\n",(0,a.jsx)(o.li,{children:"It always activates the Datacoves Airflow virtualenv."}),"\n",(0,a.jsxs)(o.li,{children:["If 'dbt_packages' isn't found, it'll run ",(0,a.jsx)(o.code,{children:"dbt deps"})," before the desired command"]}),"\n",(0,a.jsx)(o.li,{children:"It runs dbt commands inside the dbt Project Root, not the Repository root."}),"\n"]}),"\n",(0,a.jsx)(o.p,{children:"Params:"}),"\n",(0,a.jsxs)(o.ul,{children:["\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"bash_command"}),": command to run"]}),"\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"project_dir"})," (optional): relative path from repo root to a specific dbt project."]}),"\n"]}),"\n",(0,a.jsx)(o.pre,{children:(0,a.jsx)(o.code,{className:"language-python",children:'import datetime\n\nfrom airflow.decorators import dag\nfrom operators.datacoves.dbt import DatacovesDbtOperator\n\n\n@dag(\n    default_args={\n        "start_date": datetime.datetime(2023, 1, 1, 0, 0),\n        "owner": "Noel Gomez",\n        "email": "gomezn@example.com",\n        "email_on_failure": True,\n    },\n    description="Sample DAG for dbt build",\n    schedule_interval="0 0 1 */12 *",\n    tags=["version_2"],\n    catchup=False,\n)\ndef yaml_dbt_dag():\n    run_dbt = DatacovesDbtOperator(\n        task_id="run_dbt", bash_command="dbt run -s personal_loans"\n    )\n\nyaml_dbt_dag()\n'})}),"\n",(0,a.jsx)(o.h2,{id:"data-sync-operators",children:"Data Sync Operators"}),"\n",(0,a.jsx)(o.p,{children:"To synchronize the Airflow database, we can use an Airflow DAG with one of the Airflow operators below."}),"\n",(0,a.jsxs)(o.p,{children:["Datacoves has the following Airflow Data Sync Operators: ",(0,a.jsx)(o.code,{children:"DatacovesDataSyncOperatorSnowflake"})," and ",(0,a.jsx)(o.code,{children:"DatacovesDataSyncOperatorRedshift"}),"."]}),"\n",(0,a.jsx)(o.p,{children:"Both of them receive the same arguments, so we won't differentiate examples. Select the appropriate provider for your Data Warehouse."}),"\n",(0,a.jsxs)(o.blockquote,{children:["\n",(0,a.jsxs)(o.p,{children:["[!NOTE]To avoid synchronizing unnecessary Airflow tables, the following Airflow tables are synced by default: ",(0,a.jsx)(o.code,{children:"ab_permission"}),", ",(0,a.jsx)(o.code,{children:"ab_role"}),", ",(0,a.jsx)(o.code,{children:"ab_user"}),", ",(0,a.jsx)(o.code,{children:"dag"}),", ",(0,a.jsx)(o.code,{children:"dag_run"}),", ",(0,a.jsx)(o.code,{children:"dag_tag"}),", ",(0,a.jsx)(o.code,{children:"import_error"}),", ",(0,a.jsx)(o.code,{children:"job"}),", ",(0,a.jsx)(o.code,{children:"task_fail"}),", ",(0,a.jsx)(o.code,{children:"task_instance"})]}),"\n"]}),"\n",(0,a.jsx)(o.p,{children:"These operators can receive:"}),"\n",(0,a.jsxs)(o.ul,{children:["\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"tables"}),": a list of tables to override the default ones. ",(0,a.jsx)(o.em,{children:"Warning:"})," An empty list ",(0,a.jsx)(o.code,{children:"[]"})," will perform a full-database sync."]}),"\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"additional_tables"}),": a list of additional tables you would want to add to the default set."]}),"\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"destination_schema"}),": the destination schema where the Airflow tables will end-up. By default, the schema will be named as follows: ",(0,a.jsx)(o.code,{children:"airflow-{datacoves environment slug}"})," for example ",(0,a.jsx)(o.code,{children:"airflow-qwe123"})]}),"\n",(0,a.jsxs)(o.li,{children:[(0,a.jsx)(o.code,{children:"service_connection_name"})," The name of your environment variables from your ",(0,a.jsx)(o.a,{href:"/docusaurus-test/docs/how-tos/datacoves/how_to_service_connections",children:"service connection"})," which are automatically injected to airflow if you select ",(0,a.jsx)(o.code,{children:"Environment Variables"})," as the ",(0,a.jsx)(o.code,{children:"Delivery Mode"}),"."]}),"\n"]}),"\n",(0,a.jsx)(o.pre,{children:(0,a.jsx)(o.code,{className:"language-python",children:'"""## Datacoves Airflow db Sync Sample DAG\nThis DAG is a sample using the DatacovesDataSyncOperatorSnowflake Airflow Operator\nto sync the Airflow Database to a target db\n"""\n\nfrom airflow.decorators import dag\nfrom operators.datacoves.data_sync import DatacovesDataSyncOperatorSnowflake\n\n@dag(\n    default_args={"start_date": "2021-01"},\n    description="sync_data_script",\n    schedule_interval="0 0 1 */12 *",\n    tags=["version_3"],\n    catchup=False,\n)\ndef sync_airflow_db():\n    # service connection name default is \'airflow_db_load\'.\n    # Destination type default is \'snowflake\' (and the only one supported for now)\n    sync_data_script = DatacovesDataSyncOperatorSnowflake(\n        service_connection_name="airflow_db_load",  # this can be omitted or changed to another service connection name.\n    )\n\nsync_airflow_db()\n'})})]})}function h(e={}){const{wrapper:o}={...(0,t.R)(),...e.components};return o?(0,a.jsx)(o,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}}}]);