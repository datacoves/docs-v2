"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6664],{7220:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/airflow_adf_connection-247980868af475356ab1ba406ee9fa25.png"},28453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(96540);const t={},o=i.createContext(t);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(o.Provider,{value:n},e.children)}},47084:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/admin-connections-8f6e6d9caa31fe197b2f95b1b55f0de8.png"},88948:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"how-tos/airflow/run-adf-pipeline","title":"DAGs - Run ADF Pipelines","description":"You can use Airflow in Datacoves to trigger a Microsoft Azure Data Factory pipeline. This guide will walk you through the configuration process.","source":"@site/docs/how-tos/airflow/run-adf-pipeline.md","sourceDirName":"how-tos/airflow","slug":"/how-tos/airflow/run-adf-pipeline","permalink":"/docusaurus-test/docs/how-tos/airflow/run-adf-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/how-tos/airflow/run-adf-pipeline.md","tags":[],"version":"current","sidebarPosition":26,"frontMatter":{"title":"DAGs - Run ADF Pipelines","sidebar_position":26},"sidebar":"tutorialSidebar","previous":{"title":"DAGS - Load from S3 to Snowflake","permalink":"/docusaurus-test/docs/how-tos/airflow/s3-to-snowflake"},"next":{"title":"DAGs - Run Airbyte sync jobs","permalink":"/docusaurus-test/docs/how-tos/airflow/run-airbyte-sync-jobs"}}');var t=r(74848),o=r(28453);const s={title:"DAGs - Run ADF Pipelines",sidebar_position:26},a="Use Microsoft Azure Data Factory Operators",c={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"How to get the ADF information",id:"how-to-get-the-adf-information",level:3},{value:"Create a Microsoft Azure Data Factory Connection in Airflow",id:"create-a-microsoft-azure-data-factory-connection-in-airflow",level:2},{value:"Example DAG",id:"example-dag",level:2},{value:"Understanding the Airflow DAG",id:"understanding-the-airflow-dag",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"use-microsoft-azure-data-factory-operators",children:"Use Microsoft Azure Data Factory Operators"})}),"\n",(0,t.jsx)(n.p,{children:"You can use Airflow in Datacoves to trigger a Microsoft Azure Data Factory pipeline. This guide will walk you through the configuration process."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["You will need to set up a ",(0,t.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal",children:"Microsoft Entra Application"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Assign the ",(0,t.jsx)(n.code,{children:"Data Factory Contributor"})," role to your Microsoft Entra Application. You can do this by heading into Resource Groups and then following ",(0,t.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal#assign-a-role-to-the-application",children:"these instructions"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Collect the following values from your ADF account, more information on where to find these items in the next section:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"DATA_FACTORY_NAME"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"RESOURCE_GROUP_NAME"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"SUBSCRIPTION_ID"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"APPLICATION_CLIENT_ID"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"TENANT_ID"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"CLIENT_SECRET"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-to-get-the-adf-information",children:"How to get the ADF information"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 1:"}),"  Login to your Microsoft Azure console and navigate to the ",(0,t.jsx)(n.a,{href:"https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.DataFactory%2FdataFactories",children:"Data Factories service"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 2:"})," Copy the",(0,t.jsx)(n.code,{children:"DATA_FACTORY_NAME"})," for the factory which holds your data pipeline."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 3:"})," Open the factory and copy the ",(0,t.jsx)(n.code,{children:"RESOURCE_GROUP_NAME"}),", and the ",(0,t.jsx)(n.code,{children:"SUBSCRIPTION_ID"})," from the Overview tab"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 4:"})," Navigate to the Microsoft Entra ID service, click on number next to ",(0,t.jsx)(n.em,{children:"Applicaitons"})," on the Overview tab. Next click the ",(0,t.jsx)(n.em,{children:"All Applications"})," tab, open the application or register a new application then open it and copy the ",(0,t.jsx)(n.code,{children:"APPLICATION_CLIENT_ID"})," and Directory",(0,t.jsx)(n.code,{children:"TENANT_ID"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 5:"})," Click on ",(0,t.jsx)(n.em,{children:"Certificates and Secrets"})," and generate a new secret for your Microsoft Entra Application and copy the ",(0,t.jsx)(n.em,{children:"Value"}),". This is the ",(0,t.jsx)(n.code,{children:"CLIENT_SECRET"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"create-a-microsoft-azure-data-factory-connection-in-airflow",children:"Create a Microsoft Azure Data Factory Connection in Airflow"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 1:"})," In Datacoves, a user with the ",(0,t.jsx)(n.code,{children:"securityadmin"})," role must go to the ",(0,t.jsx)(n.code,{children:"Airflow Admin -> Connection"})," menu."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Airflow Connection",src:r(47084).A+"",width:"1126",height:"468"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 2:"})," Create a new connection using the following details."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Connection Id:"})," ",(0,t.jsx)(n.code,{children:"azure_data_factory_default"})," - this name will be used in the Airflow DAG and is the default name used by the ADF Operator"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Connection Type:"})," ",(0,t.jsx)(n.code,{children:"Azure Data Factory"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Client ID:"})," Your ",(0,t.jsx)(n.code,{children:"APPLICATION_CLIENT_ID"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Secret:"}),"  Your ",(0,t.jsx)(n.code,{children:"CLIENT_SECRET"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tenant ID:"})," Your ",(0,t.jsx)(n.code,{children:"TENANT_ID"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Factory Name"}),": Your ",(0,t.jsx)(n.code,{children:"DATA_FACTORY_NAME"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Resource Group Name"}),": Your ",(0,t.jsx)(n.code,{children:"RESOURCE_GROUP_NAME"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Subscription ID:"})," Your ",(0,t.jsx)(n.code,{children:"SUBSCRIPTION_ID"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"Replace the values in the screenshot below with the actual values found above."})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"adf connection",src:r(7220).A+"",width:"2734",height:"1472"})}),"\n",(0,t.jsx)(n.h2,{id:"example-dag",children:"Example DAG"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["You will need to update the ",(0,t.jsx)(n.code,{children:"pipeline_name"}),", ",(0,t.jsx)(n.code,{children:"resource_group_name"}),", and ",(0,t.jsx)(n.code,{children:"factory_name"})," arguments below with the correct names."]})}),"\n",(0,t.jsxs)(n.p,{children:["Once you have configured your Databricks connection and variables, you are ready to create your DAG. Head into the ",(0,t.jsx)(n.code,{children:"Transform"})," tab to begin writing your DAG inside ",(0,t.jsx)(n.code,{children:"orchestrate/dags"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'\n"""Example Airflow Azure Data Factory DAG."""\n\nfrom datetime import datetime\nfrom airflow.decorators import dag, task_group\nfrom airflow.providers.microsoft.azure.operators.data_factory import AzureDataFactoryRunPipelineOperator\nfrom airflow.providers.microsoft.azure.sensors.data_factory import AzureDataFactoryPipelineRunStatusSensor\n\n@dag(\n    schedule="@daily",\n    start_date=datetime(2024, 1, 1),\n    tags=["version_1"],\n    catchup=False,\n    default_args={\n        "azure_data_factory_conn_id": "azure_data_factory_default",\n        "factory_name": "your-factory-name", \n        "resource_group_name": "your-resource-name",\n    },\n)\ndef adf_example_run():\n    """Run an Azure Data Factory pipeline with async status checking."""\n\n    @task_group(group_id="adf_pipeline_group", tooltip="ADF Pipeline Group")\n    def adf_pipeline_tasks():\n        run_pipeline = AzureDataFactoryRunPipelineOperator(\n            task_id="run_pipeline",\n            pipeline_name="myTestPipeline",  # Rename to your Pipeline name\n            parameters={"myParam": "value"},\n            wait_for_termination=False,\n        )\n\n        # Deferrable sensor for async pipeline status checking\n        pipeline_run_async_sensor = AzureDataFactoryPipelineRunStatusSensor(\n            task_id="pipeline_run_async_sensor",\n            run_id=run_pipeline.output["run_id"],\n            deferrable=True,\n        )\n\n        run_pipeline >> pipeline_run_async_sensor\n\n    adf_pipeline_group = adf_pipeline_tasks()\n\nDAG = adf_example_run()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"understanding-the-airflow-dag",children:"Understanding the Airflow DAG"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Understand how to ",(0,t.jsx)(n.a,{href:"https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/connections/adf.html",children:"authenticate with ADF"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["The DAG makes use of the ",(0,t.jsx)(n.a,{href:"https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/operators/adf_run_pipeline.html",children:"AzureDataFactoryRunPipelineOperator"})," to run an Azure Data Factory pipeline. It also shows how it\u2019s possible to pass parameters that can be used in the pipeline."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);