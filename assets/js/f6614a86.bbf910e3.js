"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2787],{6787:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/datasets_graph-db1f13d1ef214929a2278f8f2e7d2849.png"},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var a=t(96540);const r={},i=a.createContext(r);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(i.Provider,{value:n},e.children)}},40840:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"how-tos/airflow/api-triggered-dag","title":"Airflow - Trigger a DAG using Datasets","description":"Overview","source":"@site/docs/how-tos/airflow/api-triggered-dag.md","sourceDirName":"how-tos/airflow","slug":"/how-tos/airflow/api-triggered-dag","permalink":"/docusaurus-test/docs/how-tos/airflow/api-triggered-dag","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Airflow - Trigger a DAG using Datasets","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Airflow - Sync Internal Airflow database","permalink":"/docusaurus-test/docs/how-tos/airflow/sync-database"},"next":{"title":"Airflow - Use Key-Pair Authentication","permalink":"/docusaurus-test/docs/how-tos/airflow/use-key-pair-authentication"}}');var r=t(74848),i=t(28453);const s={title:"Airflow - Trigger a DAG using Datasets",sidebar_position:6},o="How to Trigger a DAG using Datasets",d={},l=[{value:"Overview",id:"overview",level:2},{value:"Producer DAG",id:"producer-dag",level:2},{value:"Lambda Function",id:"lambda-function",level:2},{value:"Creating your zip files",id:"creating-your-zip-files",level:3},{value:"Create a Lambda Function",id:"create-a-lambda-function",level:3},{value:"Set Environment Variables",id:"set-environment-variables",level:3},{value:"Configuring the S3 Event Notification",id:"configuring-the-s3-event-notification",level:2},{value:"Setting Up the Airflow DAG",id:"setting-up-the-airflow-dag",level:2},{value:"Example DAG",id:"example-dag",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"how-to-trigger-a-dag-using-datasets",children:"How to Trigger a DAG using Datasets"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This guide explains how to trigger Airflow DAGs with Datasets. DAGs can be triggered by another DAG using datasets or by an external process that sends a dataset event using the Airflow API."}),"\n",(0,r.jsx)(n.h2,{id:"producer-dag",children:"Producer DAG"}),"\n",(0,r.jsx)(n.p,{children:"Airflow enables DAGs to be triggered dynamically based on dataset updates. A producer DAG updates a dataset, automatically triggering any consumer DAGs subscribed to it."}),"\n",(0,r.jsx)(n.p,{children:"To implement this, start by creating a DAG and defining the dataset it will update."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# data_aware_producer_dag.py\nimport datetime\n\nfrom airflow.decorators import dag, task\nfrom airflow.datasets import Dataset\n\n\n# A dataset can be anything, it will be a poiner in the Airflow db.\n# If you need to access url like s3://my_bucket/my_file.txt then you can set\n# it with the proper path for reuse.\nDAG_UPDATED_DATASET = Dataset("upstream_data")\n\n@dag(\n    default_args={\n        "start_date": datetime.datetime(2024, 1, 1, 0, 0),\n        "owner": "Noel Gomez",\n        "email": "gomezn@example.com",\n        "retries": 3\n    },\n    description="Sample Producer DAG",\n    schedule="0 0 1 */12 *",\n    tags=["extract_and_load"],\n    catchup=False,\n)\ndef data_aware_producer_dag():\n    @task(outlets=[DAG_UPDATED_DATASET])\n    def extract_and_load_dlt():\n        print("I\'m the producer")\n\n    extract_and_load_dlt()\n\n\ndag = data_aware_producer_dag()\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Thats it, now you are ready to create your ",(0,r.jsx)(n.a,{href:"#setting-up-the-airflow-dag",children:"Consumer DAG"})]}),"\n",(0,r.jsx)(n.h2,{id:"lambda-function",children:"Lambda Function"}),"\n",(0,r.jsxs)(n.p,{children:["Alternatively, you can trigger a DAG externally using the ",(0,r.jsx)(n.a,{href:"/docusaurus-test/docs/how-tos/airflow/use-airflow-api",children:"Airflow API"}),". In this example we will be using an AWS Lambda Function to trigger your DAG once data lands in an S3 Bucket."]}),"\n",(0,r.jsx)(n.h3,{id:"creating-your-zip-files",children:"Creating your zip files"}),"\n",(0,r.jsxs)(n.p,{children:["To run your python script in a lambda function you need to upload the ",(0,r.jsx)(n.code,{children:"requests"})," library\nalong with your ",(0,r.jsx)(n.code,{children:"lambda_function.py"})," file."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Create a python file locally and write out your function. Below is an example function."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Lambda function:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import requests\nimport os\nimport json\n\n# In Lambda, environment variables are set in the Lambda configuration\n# rather than using dotenv\nAPI_URL = os.environ.get(\"AIRFLOW_API_URL\")\nAPI_KEY = os.environ.get(\"DATACOVES_API_KEY\")\n\ndef update_dataset(dataset_name):\n    url = f\"{API_URL}/datasets/events\"\n\n    response = requests.post(\n        url=url,\n        headers={\n            \"Authorization\": f\"Token {API_KEY}\",\n        },\n        json={\"dataset_uri\": dataset_name,}\n    )\n\n    try:\n        return response.json()\n    except ValueError:\n        return response.text\n\ndef print_response(response):\n    if response:\n        msg = json.dumps(response, indent=2)\n        print(f\"Event posted successfully:\\n{'='*30}\\n\\n {msg}\")\n\ndef lambda_handler(event, context):\n    print(\"Lambda execution started\")\n\n    try:\n        print(f\"Environment variables: API_URL={API_URL is not None}, API_KEY={API_KEY is not None}\")\n\n        # Extract S3 information\n        bucket = event['Records'][0]['s3']['bucket']['name']\n        key = event['Records'][0]['s3']['object']['key']\n        print(f\"S3 event details: bucket={bucket}, key={key}\")\n\n        print(f\"File uploaded: {bucket}/{key}\")\n\n        # Airflow Dataset name must be static so if filename changes, that would have to\n        # be addressed above\n        dataset_name = f\"s3://{bucket}/{key}\"\n\n        response = update_dataset(dataset_name)\n        print_response(response)\n\n        return {\n            'statusCode': 200,\n            'body': 'Successfully processed S3 event'\n        }\n    except Exception as e:\n        print(f\"ERROR: {str(e)}\")\n        import traceback\n        print(traceback.format_exc())\n        return {\n            'statusCode': 500,\n            'body': f'Error: {str(e)}'\n        }\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Run the following commands locally to prepare a zip file with everything you need."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install --target ./package requests\ncd package\nzip -r ../deployment-package.zip .\ncd ..\nzip -g deployment-package.zip lambda_function.py\n"})}),"\n",(0,r.jsx)(n.h3,{id:"create-a-lambda-function",children:"Create a Lambda Function"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Create a new AWS lambda function."}),"\n",(0,r.jsx)(n.li,{children:"Set the runtime to Python 3.10."}),"\n",(0,r.jsxs)(n.li,{children:["Create an IAM role and add the following policy:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AmazonS3ReadOnlyAccess to bucket"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Upload ",(0,r.jsx)(n.code,{children:"deployment-package.zip"})," from the earlier step into the Lambda function."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"set-environment-variables",children:"Set Environment Variables"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Gather your ",(0,r.jsx)(n.a,{href:"/docusaurus-test/docs/how-tos/airflow/use-airflow-api#step-1-navigate-to-your-target-environment",children:"API credentials"})," Configure the following environment variables in the Lambda Function's Configuration:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"AIRFLOW_API_URL"})," (the API URL for Airflow)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"AIRFLOW_API_KEY"})," (the API key for authentication)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"configuring-the-s3-event-notification",children:"Configuring the S3 Event Notification"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Go to S3 and Open the Target Bucket"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Create a New Event Notification under the bucket's properties"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event Name:"})," ",(0,r.jsx)(n.code,{children:"TriggerAirflowDAG"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prefix (Optional):"})," Specify a subfolder if needed."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Suffix (Optional)"})," If you would like to trigger specific files ie) .csv"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event Type:"})," Select ",(0,r.jsx)(n.code,{children:"All object create events"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Destination:"})," Select ",(0,r.jsx)(n.strong,{children:"AWS Lambda"})," and choose the function created earlier."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Now you are ready to set up your Consumer DAG."}),"\n",(0,r.jsx)(n.h2,{id:"setting-up-the-airflow-dag",children:"Setting Up the Airflow DAG"}),"\n",(0,r.jsxs)(n.p,{children:["Whether you decide to use a producer DAG or the Airflow API, the last step is to create an Airflow DAG that is triggered by a dataset event rather than a schedule. This particular example can be triggered with either a ",(0,r.jsx)(n.code,{children:"LAMBDA_UPDATED_DATASET"})," or ",(0,r.jsx)(n.code,{children:"DAG_UPDATED_DATASET"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"datasets graph",src:t(6787).A+"",width:"1586",height:"876"})}),"\n",(0,r.jsx)(n.h3,{id:"example-dag",children:"Example DAG"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import datetime\n\nfrom airflow.decorators import dag, task\nfrom airflow.datasets import Dataset\n\nLAMBDA_UPDATED_DATASET = Dataset("s3://my_bucket/my_folder/my_file.csv")\nDAG_UPDATED_DATASET = Dataset("upstream_data")\n\n@dag(\n    default_args={\n        "start_date": datetime.datetime(2024, 1, 1, 0, 0),\n        "owner": "Noel Gomez",\n        "email": "gomezn@example.com",\n        "retries": 1\n    },\n    description="Sample Producer DAG",\n    schedule=(LAMBDA_UPDATED_DATASET | DAG_UPDATED_DATASET),\n    tags=["transform"],\n    catchup=False,\n)\ndef data_aware_consumer_dag():\n    @task\n    def run_consumer():\n        print("I\'m the consumer")\n\n    run_consumer()\n\n\ndag = data_aware_consumer_dag()\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"Ensure the Dataset you are sending an event to exists in Airflow. It will be created automatically when a DAG is created. If a dataset does not exist when the API event is sent, the API call will fail."})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);